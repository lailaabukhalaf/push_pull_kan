Despite recent advances in artificial neural network architectures, most models still rely on static neuron activation functions, limiting their flexibility, and interpretability. Kolmogorovâ€“Arnold Networks (KANs) offer an alternative approach by introducing learnable spline functions that enable smooth, localised and learnable activation behaviours. However, training KANs and their variants still treat synaptic weights and neuron activations independently, missing the opportunity for co-adaptive learning. This research proposes a novel architecture, inspired by neuroscience, where weights and spline activations interact through a dynamic feedback loop adapting to each other in real time. As the weights evolve, the spline shapes respond appropriately by reshaping activation regions based on gradient flow and data-driven relevance. We aim to explore whether this mutual adaptation improves generalisation, accelerates convergence, reduces memory complexity, and enhances robustness to noise and overfitting while maintaining or exceeding the performance and interpretability of standard KANs. The model is expected to self-tune and organise around key input regions without manual grid design, uncovering hidden structures. This research envisions a new class of intelligent, self-adaptive, and mutually aware neural systems pushing the boundaries to the next generation of deep learning. <img width="468" height="381" alt="image" src="https://github.com/user-attachments/assets/be5cb215-4caf-415a-a758-a8bcc0e6ff5a" />
