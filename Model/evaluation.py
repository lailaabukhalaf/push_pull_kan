# -*- coding: utf-8 -*-
"""Evaluation.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1a3j4qDnmtnnUBqAx8P4pWn6fxN4aPLMo
"""

# 4. Gradient Norms Tracking
def compute_grad_norms(model, param_group_map=None):
    grad_norms = {}
    for name, param in model.named_parameters():
        if param.grad is not None:
            group = None
            if param_group_map:
                for g, substrings in param_group_map.items():
                    if any(s in name for s in substrings):
                        group = g
                        break
            group = group or "all"
            grad_norms[group] = grad_norms.get(group, 0.0) + param.grad.detach().norm().item()
    return grad_norms

# 6. Throughput
def measure_throughput(model, X, batch_size=64, device=None, n_warmup=2, n_repeat=20):
    if device is None: device = next(model.parameters()).device
    model.eval()
    X = X.to(device)
    with torch.no_grad():
        # Warmup
        for _ in range(n_warmup):
            _ = model(X[:batch_size])
        # Main timing
        start = time.time()
        n_pred = 0
        for _ in range(n_repeat):
            for i in range(0, len(X), batch_size):
                _ = model(X[i:i+batch_size])
                n_pred += min(batch_size, len(X)-i)
        elapsed = time.time() - start
    throughput = n_pred / elapsed
    print(f"Throughput: {throughput:.2f} samples/second")
    return throughput

# 7. Model Size
def model_size(model, fname="tmp_model.pt"):
    torch.save(model.state_dict(), fname)
    size = os.path.getsize(fname) / 1024
    os.remove(fname)
    n_params = sum(p.numel() for p in model.parameters())
    print(f"Model params: {n_params:,}  |  Disk size: {size:.1f} KB")
    return n_params, size

# ==== 5. Metrics, confusion matrix, evaluation ====
def evaluate_with_confusion(model, dataset, criterion, device=None, print_output=True):
    if device is None:
        device = next(model.parameters()).device
    if isinstance(device, str):
        device = torch.device(device)
    model.eval()
    results = {}
    process = psutil.Process(os.getpid())
    for split in ['train', 'test']:
        input_key = f"{split}_input"
        label_key = f"{split}_label"
        if input_key not in dataset or label_key not in dataset: continue
        xb = dataset[input_key].to(device)
        yb = dataset[label_key].to(device)
        mem_before = process.memory_info().rss / (1024 ** 2)
        if device.type == 'cuda': torch.cuda.reset_peak_memory_stats(device); torch.cuda.synchronize()
        start = time.time()
        with torch.no_grad():
            logits = model(xb)
            preds = logits.argmax(1)
            acc = (preds == yb).float().mean().item()
            loss = criterion(logits, yb).item()
            preds_np = preds.cpu().numpy()
            labels_np = yb.cpu().numpy()
            f1_micro = f1_score(labels_np, preds_np, average='micro')
            f1_macro = f1_score(labels_np, preds_np, average='macro', zero_division=0)
            f1_weighted = f1_score(labels_np, preds_np, average='weighted')
            precision = precision_score(labels_np, preds_np, average='macro', zero_division=0)
            recall = recall_score(labels_np, preds_np, average='macro', zero_division=0)
            cmatrix = confusion_matrix(labels_np, preds_np)
        end = time.time()
        elapsed = end - start
        if device.type == 'cuda':
            torch.cuda.synchronize()
            mem_used = torch.cuda.max_memory_allocated(device) / (1024 ** 2)
        else:
            mem_after = process.memory_info().rss / (1024 ** 2)
            mem_used = mem_after - mem_before
        results[split] = {
            "loss": loss, "accuracy": acc,
            "f1_micro": f1_micro, "f1_macro": f1_macro, "f1_weighted": f1_weighted,
            "precision": precision, "recall": recall,
            "confusion_matrix": cmatrix, "elapsed_time": elapsed, "memory": mem_used
        }
        if print_output:
            print(f"{split.title()} | Loss: {loss:.4f} | Acc: {acc:.4f} | "
                  f"F1 (micro): {f1_micro:.4f} | F1 (macro): {f1_macro:.4f} | "
                  f"Precision: {precision:.4f} | Recall: {recall:.4f} | "
                  f"Time: {elapsed:.3f}s | Memory: {mem_used:.1f}MB")
            print(f"{split.title()} Confusion Matrix:\n{cmatrix}\n")
    return results

pip install psutil

import psutil

from sklearn.metrics import precision_recall_fscore_support
def per_class_metrics(y_true, y_pred, class_names=None, verbose=True):
    p, r, f1, support = precision_recall_fscore_support(y_true, y_pred, average=None, zero_division=0)
    accs = []
    classes = np.unique(y_true)
    for i in classes:
        idx = y_true == i
        accs.append(accuracy_score(y_true[idx], y_pred[idx]))
    if verbose:
        print("Per-class metrics:")
        for i, cls in enumerate(classes):
            label = str(cls) if class_names is None else class_names[cls]
            print(f"Class {label}: Precision={p[i]:.3f} Recall={r[i]:.3f} F1={f1[i]:.3f} Acc={accs[i]:.3f} Support={support[i]}")
    return {"precision": p, "recall": r, "f1": f1, "accuracy": np.array(accs), "support": support}

def measure_inference_time(model, X, batch_size=64, device=None, n_warmup=5, n_repeat=50):
    """
    Measure inference time for a model.

    Args:
        model: PyTorch model
        X: Input data
        batch_size: Batch size for inference
        device: Device to run on (defaults to model's device)
        n_warmup: Number of warmup iterations
        n_repeat: Number of measurement iterations

    Returns:
        avg_time: Average time per sample in milliseconds
        total_time: Total time for all samples in seconds
    """
    if device is None:
        device = next(model.parameters()).device

    model.eval()
    X = X.to(device)
    batch = X[:batch_size]

    # Warmup
    with torch.no_grad():
        for _ in range(n_warmup):
            _ = model(batch)

    # Timing
    start = time.time()
    with torch.no_grad():
        for _ in range(n_repeat):
            _ = model(batch)
    end = time.time()

    total_time = end - start
    avg_time = (total_time / n_repeat / batch_size) * 1000  # ms per sample

    print(f"Inference time: {avg_time:.3f} ms per sample")
    print(f"Total time for {n_repeat} batches: {total_time:.3f} seconds")

    return avg_time, total_time

def measure_training_time(model, criterion, optimizer, X, y, batch_size=64, device=None, n_warmup=2, n_repeat=10):
    """
    Measure training time for a model.

    Args:
        model: PyTorch model
        criterion: Loss function
        optimizer: Optimizer
        X: Input data
        y: Target data
        batch_size: Batch size for training
        device: Device to run on (defaults to model's device)
        n_warmup: Number of warmup iterations
        n_repeat: Number of measurement iterations

    Returns:
        avg_time: Average time per batch in milliseconds
        total_time: Total time for all batches in seconds
    """
    if device is None:
        device = next(model.parameters()).device

    model.train()
    X_batch = X[:batch_size].to(device)
    y_batch = y[:batch_size].to(device)

    # Warmup
    for _ in range(n_warmup):
        optimizer.zero_grad()
        outputs = model(X_batch)
        loss = criterion(outputs, y_batch)
        loss.backward()
        optimizer.step()

    # Timing
    start = time.time()
    for _ in range(n_repeat):
        optimizer.zero_grad()
        outputs = model(X_batch)
        loss = criterion(outputs, y_batch)
        loss.backward()
        optimizer.step()
    end = time.time()

    total_time = end - start
    avg_time = (total_time / n_repeat) * 1000  # ms per batch

    print(f"Training time: {avg_time:.3f} ms per batch")
    print(f"Total time for {n_repeat} batches: {total_time:.3f} seconds")

    return avg_time, total_time

def measure_memory_usage(model, X, y=None, criterion=None, optimizer=None,
                         batch_size=64, device='cuda', mode='inference'):
    """
    Measure peak memory usage during inference or training.

    Args:
        model: PyTorch model
        X: Input data
        y: Target data (only needed for training mode)
        criterion: Loss function (only needed for training mode)
        optimizer: Optimizer (only needed for training mode)
        batch_size: Batch size
        device: Device to run on (must be CUDA)
        mode: 'inference' or 'training'

    Returns:
        memory_used: Peak memory used in MB
    """
    if 'cuda' not in str(device):
        print("Memory measurement requires CUDA device")
        return None

    # Move model to device if it's not already there
    model = model.to(device)

    # Reset peak memory stats
    reset_peak_memory_stats(device)

    X_batch = X[:batch_size].to(device)

    if mode == 'inference':
        model.eval()
        with torch.no_grad():
            _ = model(X_batch)
    elif mode == 'training':
        if y is None or criterion is None or optimizer is None:
            print("Training mode requires y, criterion, and optimizer")
            return None

        model.train()
        y_batch = y[:batch_size].to(device)
        optimizer.zero_grad()
        outputs = model(X_batch)
        loss = criterion(outputs, y_batch)
        loss.backward()
        optimizer.step()

    # Get peak memory in MB
    memory_used = max_memory_allocated(device) / (1024 * 1024)

    print(f"Peak memory usage ({mode}): {memory_used:.2f} MB")

    return memory_used

def profile_model_comprehensive(model, X, y, criterion, optimizer, batch_size=64, device=None):
    """
    Comprehensive profiling of a model - size, time, and memory.

    Args:
        model: PyTorch model
        X: Input data
        y: Target data
        criterion: Loss function
        optimizer: Optimizer
        batch_size: Batch size
        device: Device to run on

    Returns:
        results: Dictionary containing all profiling results
    """
    if device is None:
        device = next(model.parameters()).device

    results = {}

    # Model size
    n_params, disk_size = model_size(model)
    results['params'] = n_params
    results['disk_size_kb'] = disk_size

    # Inference time
    inf_time_per_sample, inf_total_time = measure_inference_time(
        model, X, batch_size, device)
    results['inference_time_ms'] = inf_time_per_sample

    # Training time
    train_time_per_batch, train_total_time = measure_training_time(
        model, criterion, optimizer, X, y, batch_size, device)
    results['training_time_ms'] = train_time_per_batch

    # Memory usage (only if CUDA device)
    if 'cuda' in str(device):
        inf_memory = measure_memory_usage(
            model, X, mode='inference', batch_size=batch_size, device=device)
        results['inference_memory_mb'] = inf_memory

        train_memory = measure_memory_usage(
            model, X, y, criterion, optimizer,
            batch_size=batch_size, device=device, mode='training')
        results['training_memory_mb'] = train_memory

    # Throughput
    throughput = measure_throughput(model, X, batch_size, device)
    results['throughput_samples_per_sec'] = throughput

    return results

def model_size(model, fname="tmp_model.pt"):
    """Calculate model size in parameters and disk space"""
    torch.save(model.state_dict(), fname)
    size = os.path.getsize(fname) / 1024
    os.remove(fname)
    n_params = sum(p.numel() for p in model.parameters())
    print(f"Model params: {n_params:,}  |  Disk size: {size:.1f} KB")
    return n_params, size

def measure_throughput(model, X, batch_size=64, device=None, n_warmup=2, n_repeat=20):
    """Calculate model throughput in samples per second"""
    if device is None: device = next(model.parameters()).device
    model.eval()
    X = X.to(device)
    with torch.no_grad():
        # Warmup
        for _ in range(n_warmup):
            _ = model(X[:batch_size])
        # Main timing
        start = time.time()
        n_pred = 0
        for _ in range(n_repeat):
            for i in range(0, len(X), batch_size):
                _ = model(X[i:i+batch_size])
                n_pred += min(batch_size, len(X)-i)
        elapsed = time.time() - start
    throughput = n_pred / elapsed
    print(f"Throughput: {throughput:.2f} samples/second")
    return throughput

def plot_profiling_results(results_dict, title="Model Profiling Results"):
    """Plot profiling results in a comprehensive visualization"""
    metrics = list(results_dict.keys())
    values = list(results_dict.values())

    # Separate metrics by type
    time_metrics = [m for m in metrics if 'time' in m or 'throughput' in m]
    memory_metrics = [m for m in metrics if 'memory' in m or 'size' in m]
    other_metrics = [m for m in metrics if m not in time_metrics and m not in memory_metrics]

    fig = plt.figure(figsize=(15, 10))

    # Time metrics
    if time_metrics:
        ax1 = fig.add_subplot(2, 1, 1)
        time_values = [results_dict[m] for m in time_metrics]
        ax1.bar(time_metrics, time_values, color='skyblue')
        ax1.set_ylabel('Time (ms) / Throughput')
        ax1.set_title('Time Metrics')
        for i, v in enumerate(time_values):
            ax1.text(i, v, f"{v:.2f}", ha='center', va='bottom')

    # Memory metrics
    if memory_metrics:
        ax2 = fig.add_subplot(2, 1, 2)
        memory_values = [results_dict[m] for m in memory_metrics]
        ax2.bar(memory_metrics, memory_values, color='lightgreen')
        ax2.set_ylabel('Memory (MB) / Size (KB)')
        ax2.set_title('Memory Metrics')
        for i, v in enumerate(memory_values):
            ax2.text(i, v, f"{v:.2f}", ha='center', va='bottom')

    plt.suptitle(title)
    plt.tight_layout()
    plt.subplots_adjust(top=0.9)

    return fig

import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import confusion_matrix, classification_report


def analyze_misclassifications(model, X, y_true, class_names=None, device=None):
    """
    Analyze misclassifications with probabilities.

    Args:
        model: PyTorch model
        X: Input features
        y_true: Ground truth labels
        class_names: Names of classes (optional)
        device: Device to run model on
    """
    if device is None:
        device = next(model.parameters()).device

    model.eval()
    with torch.no_grad():
        # Get model predictions
        logits = model(X.to(device))
        probabilities = torch.softmax(logits, dim=1)

        # Get predicted classes
        y_pred = logits.argmax(1).cpu().numpy()
        y_true_np = y_true.cpu().numpy()

        # Find misclassified examples
        misclassified_idx = np.where(y_pred != y_true_np)[0]

        # Print results
        print(f"\nDetailed Misclassification Analysis:")
        print(f"Found {len(misclassified_idx)} misclassified examples out of {len(y_true_np)}")

        if len(misclassified_idx) > 0:
            # Print top 10 misclassifications
            print("\nTop misclassifications (highest confidence in wrong class):")

            # Calculate confidence in wrong prediction
            wrong_conf = []
            for idx in misclassified_idx:
                wrong_class = y_pred[idx]
                confidence = probabilities[idx, wrong_class].item()
                true_class_conf = probabilities[idx, y_true_np[idx]].item()
                wrong_conf.append((idx, confidence, wrong_class, y_true_np[idx], true_class_conf))

            # Sort by confidence in wrong class (descending)
            wrong_conf.sort(key=lambda x: x[1], reverse=True)

            # Print top 10 or fewer
            for i, (idx, conf, pred_class, true_class, true_conf) in enumerate(wrong_conf[:10]):
                true_name = class_names[true_class] if class_names else f"Class {true_class}"
                pred_name = class_names[pred_class] if class_names else f"Class {pred_class}"
                print(f"{i+1}. Example {idx}: Predicted {pred_name} ({conf:.2%}) instead of {true_name} (true class prob: {true_conf:.2%})")

        # Return misclassified indices for further analysis
        return misclassified_idx, y_pred, probabilities.cpu().numpy()
