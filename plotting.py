# -*- coding: utf-8 -*-
"""Plotting.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ugGUqxDAJDusWQLFHiSsV1R-Cg7C9LLr
"""

# 2. Learning Rate Schedule & EMA Plots
def plot_lr_ema_curves(lr_W, lr_C, ema_W, ema_C):
    epochs = np.arange(1, len(lr_W)+1)
    plt.figure(figsize=(12,5))
    plt.subplot(1,2,1)
    plt.plot(epochs, lr_W, label="W LR")
    plt.plot(epochs, lr_C, label="C LR")
    plt.xlabel("Epoch")
    plt.ylabel("Learning Rate")
    plt.title("Learning Rate Schedule")
    plt.legend()
    plt.grid(True)
    plt.subplot(1,2,2)
    plt.plot(epochs, ema_W, label="EMA ΔW")
    plt.plot(epochs, ema_C, label="EMA ΔC")
    plt.xlabel("Epoch")
    plt.ylabel("EMA Parameter Change")
    plt.title("EMA of Weight/Coef. Change")
    plt.legend()
    plt.grid(True)
    plt.tight_layout()
    plt.show()

# 3. Parameter Histograms
def plot_param_histograms(model, title_prefix="Model", param_groups=None):
    plt.figure(figsize=(10,5))
    idx = 1
    for name, param in model.named_parameters():
        if param_groups is None or any(g in name for g in param_groups):
            plt.subplot(1, len(list(model.parameters())), idx)
            plt.hist(param.detach().cpu().numpy().flatten(), bins=30, alpha=0.7)
            plt.title(f"{title_prefix}\n{name}")
            idx += 1
    plt.tight_layout()
    plt.show()

def plot_grad_norms(norms_W, norms_C):
    plt.figure(figsize=(8,5))
    plt.plot(norms_W, label="W Grad Norm")
    plt.plot(norms_C, label="C Grad Norm")
    plt.xlabel("Epoch")
    plt.ylabel("Gradient Norm")
    plt.title("Gradient Norms by Group")
    plt.legend()
    plt.grid(True)
    plt.show()

# 5. Train/Validation Gap
def plot_train_val_gap(train_loss, val_loss, train_acc, val_acc):
    plt.figure(figsize=(10,4))
    epochs = range(1, len(train_loss)+1)
    plt.subplot(1,2,1)
    plt.plot(epochs, train_loss, label='Train Loss')
    plt.plot(epochs, val_loss, label='Val Loss')
    plt.fill_between(epochs, train_loss, val_loss, alpha=0.2, color='orange')
    plt.title("Loss & Gap")
    plt.legend()
    plt.subplot(1,2,2)
    plt.plot(epochs, train_acc, label='Train Acc')
    plt.plot(epochs, val_acc, label='Val Acc')
    plt.fill_between(epochs, train_acc, val_acc, alpha=0.2, color='orange')
    plt.title("Accuracy & Gap")
    plt.legend()
    plt.show()

from sklearn.calibration import calibration_curve

# 9. Calibration Curves
def plot_calibration_curve(model, X, y, n_bins=10, device=None):
    if device is None: device = next(model.parameters()).device
    model.eval()
    with torch.no_grad():
        probs = torch.softmax(model(X.to(device)), dim=1).cpu().numpy()
    y_true = y.cpu().numpy()
    plt.figure(figsize=(7, 5))
    for i in range(probs.shape[1]):
        prob_true, prob_pred = calibration_curve(y_true == i, probs[:, i], n_bins=n_bins)
        plt.plot(prob_pred, prob_true, marker='o', label=f'Class {i}')
    plt.plot([0, 1], [0, 1], 'k--', lw=2)
    plt.xlabel("Mean Predicted Probability")
    plt.ylabel("Fraction of Positives")
    plt.title("Calibration Curves")
    plt.legend()
    plt.grid(True)
    plt.show()

# ==== 6. Plotting: decision boundary, learning curves ====
def plot_decision_boundary(model, X, y, title="Model decision boundary", device=None, ax=None):
    if device is None: device = next(model.parameters()).device
    if isinstance(X, torch.Tensor): X = X.cpu().numpy()
    if isinstance(y, torch.Tensor): y = y.cpu().numpy()
    x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5
    y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5
    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 300), np.linspace(y_min, y_max, 300))
    grid = np.c_[xx.ravel(), yy.ravel()]
    if X.shape[1] > 2:
        grid_full = np.zeros((grid.shape[0], X.shape[1]))
        grid_full[:, :2] = grid
        grid_full[:, 2:] = X[:, 2:].mean(axis=0)
        grid = grid_full
    grid_tensor = torch.tensor(grid, dtype=torch.float32, device=device)
    with torch.no_grad():
        logits = model(grid_tensor)
        preds = logits.argmax(1).cpu().numpy()
    if ax is None:
        plt.figure(figsize=(6, 5))
        ax = plt.gca()
    ax.contourf(xx, yy, preds.reshape(xx.shape), alpha=0.3, cmap='coolwarm')
    scatter = ax.scatter(X[:, 0], X[:, 1], c=y, s=40, edgecolor="k", cmap='coolwarm')
    ax.set_title(title)
    ax.set_xlabel("Feature 1")
    ax.set_ylabel("Feature 2")
    plt.legend(*scatter.legend_elements(), title="Classes")
    plt.tight_layout()
    return ax

def plot_training_curves(losses_train, losses_val, accuracies_train, accuracies_val, save_path=None):
    epochs = range(1, len(losses_train) + 1)
    plt.figure(figsize=(12, 5))
    plt.subplot(1, 2, 1)
    plt.plot(epochs, losses_train, label='Training Loss')
    plt.plot(epochs, losses_val, label='Validation Loss')
    plt.title('Loss Over Epochs')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.legend()
    plt.grid(True)
    plt.subplot(1, 2, 2)
    plt.plot(epochs, accuracies_train, label='Training Accuracy')
    plt.plot(epochs, accuracies_val, label='Validation Accuracy')
    plt.title('Accuracy Over Epochs')
    plt.xlabel('Epoch')
    plt.ylabel('Accuracy')
    plt.legend()
    plt.grid(True)
    plt.tight_layout()
    if save_path is not None:
        plt.savefig(save_path)
    plt.show()

def plot_confusion_matrix(y_true, y_pred, class_names=None):
    """
    Plot confusion matrix with percentages and counts.

    Args:
        y_true: Ground truth labels
        y_pred: Predicted labels
        class_names: Names of classes (optional)
    """
    # Create confusion matrix
    cm = confusion_matrix(y_true, y_pred)

    # Create figure
    plt.figure(figsize=(10, 8))

    # Plot using seaborn for better styling
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                xticklabels=class_names, yticklabels=class_names)

    # Count correct and incorrect predictions
    correct = np.sum(np.diag(cm))
    total = np.sum(cm)
    accuracy = correct / total
    incorrect = total - correct

    plt.title(f'Confusion Matrix\nAccuracy: {accuracy:.2%} ({correct}/{total})\nMisclassified: {incorrect}/{total}')
    plt.ylabel('True label')
    plt.xlabel('Predicted label')
    plt.tight_layout()

    # Print detailed metrics
    print(f"Total predictions: {total}")
    print(f"Correct predictions: {correct} ({accuracy:.2%})")
    print(f"Incorrect predictions: {incorrect} ({1-accuracy:.2%})")

    # Return prediction statistics
    return {
        "total": total,
        "correct": correct,
        "incorrect": incorrect,
        "accuracy": accuracy
    }

import numpy as np
import matplotlib.pyplot as plt
import torch

def plot_kan_splines(model, feature_names=None, output_names=None, num_points=1000, save_path=None):
    """
    Plot splines from KAN layers

    Args:
        model: KAN model instance
        feature_names: Names of input features
        output_names: Names of output classes
        num_points: Number of points for plotting
        save_path: Path to save the figure
    """
    if feature_names is None:
        feature_names = [f'Feature {i+1}' for i in range(4)]

    if output_names is None:
        output_names = ['Setosa', 'Versicolor', 'Virginica']

    # Setup device
    device = next(model.parameters()).device

    # Put model in eval mode
    model.eval()

    # Create figure
    fig = plt.figure(figsize=(15, 12))

    # First layer: 4 input features to 8 outputs
    print("Plotting first layer splines (4 features -> 8 outputs)")

    # For each input dimension
    for in_dim in range(4):
        # Create a subplot
        ax = fig.add_subplot(2, 2, in_dim + 1)

        # Generate input data - vary only one dimension
        x_range = np.linspace(-1, 1, num_points)

        # Create input tensor where only the current dimension varies
        inputs = torch.zeros((num_points, 4), device=device)

        # Plot spline outputs for each output dimension
        outputs = []
        for i, x in enumerate(x_range):
            # Set the current dimension to x
            inputs[i, in_dim] = x

        # Get first layer activations
        with torch.no_grad():
            layer1_outputs, spline_outputs, *_ = model.kan1(inputs)

            # Plot each output dimension
            for out_dim in range(8):
                ax.plot(x_range, layer1_outputs[:, out_dim].cpu().numpy(),
                         label=f'Out {out_dim+1}' if in_dim == 0 else None)

        ax.set_title(f'{feature_names[in_dim]} Splines')
        ax.set_xlabel(feature_names[in_dim])
        ax.set_ylabel('Activation')
        ax.grid(True)

        # Only show legend for the first subplot to avoid clutter
        if in_dim == 0:
            ax.legend(loc='upper right', fontsize='small')

    # Add overall title
    plt.suptitle('KAN Spline Activations', fontsize=16)
    plt.tight_layout(rect=[0, 0, 1, 0.96])  # Adjust for suptitle

    if save_path:
        plt.savefig(save_path, dpi=300, bbox_inches='tight')

    return fig
    
def plot_coupling_scatter(lr_W, lr_C, ema_W, ema_C):
    """
    Creates scatter plots visualizing the push-pull coupling between weights and splines.
    
    Args:
        lr_W (list): Weight learning rates over epochs
        lr_C (list): Spline coefficient learning rates over epochs
        ema_W (list): EMA of weight changes over epochs
        ema_C (list): EMA of spline coefficient changes over epochs
        
    Returns:
        matplotlib figure: The created figure containing the plots
    """
    plt.figure(figsize=(12, 6))

    # Left subplot: How spline changes affect weight LR
    plt.subplot(121)
    plt.scatter(ema_C, lr_W, alpha=0.7, c=range(len(lr_W)), cmap='viridis')
    plt.xlabel("EMA of Spline Changes (eC)")
    plt.ylabel("Weight Learning Rate (etaW)")
    plt.title("Spline Changes → Weight LR")
    plt.grid(True, alpha=0.3)
    plt.colorbar(label="Epoch")

    # Right subplot: How weight changes affect spline LR
    plt.subplot(122)
    plt.scatter(ema_W, lr_C, alpha=0.7, c=range(len(lr_C)), cmap='viridis')
    plt.xlabel("EMA of Weight Changes (eW)")
    plt.ylabel("Spline Learning Rate (etaC)")
    plt.title("Weight Changes → Spline LR")
    plt.grid(True, alpha=0.3)
    plt.colorbar(label="Epoch")

    plt.tight_layout()
    return plt.gcf()

