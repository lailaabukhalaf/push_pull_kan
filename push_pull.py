# -*- coding: utf-8 -*-
"""Push_pull

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1GyGElrjEi7pkgYvjxHKyE4N-yaom2_Lm
"""

# Train with push–pull dynamic learning rates


def train_with_dynamic_lrs(model, loader, criterion,dataset,
                           base_lr_W=1e-3, base_lr_C=1e-3,
                           Y3=500, Y4=500, ema_beta=0.5, epochs=30,
                           device="cuda" if torch.cuda.is_available() else "cpu",min_lr=1e-6):
    """
    Training loop with push–pull dynamic learning rates for two parameter groups (W and C).
      - W: (scale_base, scale_sp)  — “weights”
      - C: (coef)                  — spline coefficients

      LRs are adapted per batch using EMA of parameter changes:
      eta_W = base_lr_W / (1 + exp(Y3 * EMA(ΔC)))
      eta_C = base_lr_C / (1 + exp(Y4 * EMA(ΔW)))


    Args:
        model: PyTorch model to train.
        loader: DataLoader providing (xb, yb) batches.
        criterion: Loss function.
        base_lr_W: Initial learning rate for W parameters.
        base_lr_C: Initial learning rate for C parameters.
        Y3, Y4: Sensitivity constants for learning rate adaptation.   -- Larger → the LR reacts more strongly to EMA changes.
        ema_beta: Smoothing factor for EMA of parameter deltas.
        epochs: Number of training epochs.
        device: Device to use ('cuda' or 'cpu').

    Learning rates for W and C are dynamically adjusted based on the EMA of parameter updates,
    implementing a push–pull mechanism.

         """

    model.to(device)

    # Collect parameter groups: W (scale_sp+scale_base) and C (coef params)
    W_params, C_params = collect_kan_param_groups(model)

    # Create LBFGS optimizer with initial learning rates ( will use adam optimizer in the image classsification)

    opt = torch.optim.Adam([{"params":W_params,"lr":base_lr_W},
                            {"params":C_params,"lr":base_lr_C}])

    # Create EMA trackers for parameter delta (change) magnitudes (change in weight , change in coef)
    emaW, emaC = DeltaEMA(ema_beta), DeltaEMA(ema_beta)
    with torch.no_grad():
        for p in W_params:
          emaW.prev[id(p)] = p.data.detach().cpu().clone()
        for p in C_params:
          emaC.prev[id(p)] = p.data.detach().cpu().clone()


    # for ep in range(epochs):
    #   model.train()
    #   for xb, yb in loader:
    #     xb, yb = xb.to(device), yb.to(device)
    #     opt.zero_grad()
    # # LBFGS requires a closure function
    #     def closure():
    #       opt.zero_grad()
    #       logits = model(xb)
    #       loss = criterion(logits, yb)
    #       loss.backward()
    #       return loss
    #     loss = opt.step(closure) # the final loss




        # Compute new learning rates using push–pull formulas
        # Each LR is inversely scaled by the other's delta EMA (cross-dependency)
        # etaW = max(base_lr_W / (1.0 + math.exp(Y3 * eC)), min_lr) ----- important according to the learining rate
        # etaC = max(base_lr_C / (1.0 + math.exp(Y4 * eW)), min_lr)


# For all other optimiziers like adam no need for closure we can call loss.backward() and opt.step() as usual
    losses_train, losses_val = [], []
    accs_train, accs_val = [], []
    lr_W_hist, lr_C_hist = [], []
    ema_W_hist, ema_C_hist = [], []
    grad_norms_W_hist, grad_norms_C_hist = [], []

    for ep in range(epochs):
        model.train()
        total_loss, total_correct, total_count = 0, 0, 0

        for xb,yb in loader:
            xb, yb = xb.to(device), yb.to(device)
            opt.zero_grad();
            logits = model(xb);
            loss = criterion(logits, yb);
            loss.backward();
            opt.step()
            total_loss += loss.item() * xb.size(0)
            total_correct += (logits.argmax(1) == yb).sum().item()
            total_count += xb.size(0)

        # Update EMA of parameter deltas AFTER params changed
        eW = emaW.update(W_params)   # how much W moved this batch
        eC = emaC.update(C_params)   # how much C moved this batch

        # Compute new learning rates using push–pull formulas
        # Each LR is inversely scaled by the other's delta EMA (cross-dependency)
        etaW = base_lr_W / (1.0 + math.exp(Y3 * eC))
        etaC = base_lr_C / (1.0 + math.exp(Y4 * eW))

        # Update optimizer's learning rates for each parameter group

        opt.param_groups[0]["lr"] = etaW
        opt.param_groups[1]["lr"] = etaC
 # --- Save learning rates & EMA stats ---
        lr_W_hist.append(etaW)
        lr_C_hist.append(etaC)
        ema_W_hist.append(eW)
        ema_C_hist.append(eC)

          # --- Compute & save gradient norms per group after last batch ---
        grad_norm_W = sum((p.grad.detach().norm().item() if p.grad is not None else 0) for p in W_params)
        grad_norm_C = sum((p.grad.detach().norm().item() if p.grad is not None else 0) for p in C_params)
        grad_norms_W_hist.append(grad_norm_W)
        grad_norms_C_hist.append(grad_norm_C)

        # --- Epoch train stats ---
        losses_train.append(total_loss / total_count)
        accs_train.append(total_correct / total_count)


        # Full-batch validation after epoch:
        model.eval()
        with torch.no_grad():
          X_val = dataset["test_input"].to(device)
          y_val = dataset["test_label"].to(device)
          val_logits = model(X_val)
          val_loss = criterion(val_logits, y_val).item()
          val_acc = (val_logits.argmax(1) == y_val).float().mean().item()
          losses_val.append(val_loss)
          accs_val.append(val_acc)

        print(f"epoch {ep+1}: train_loss={losses_train[-1]:.4f}  val_loss={val_loss:.4f}  "
              f"EMAW={eW:.5g} EMAC={eC:.5g}  ηW={etaW:.2e} ηC={etaC:.2e}")
    return {"losses_train": losses_train,
        "losses_val": losses_val,
        "accs_train": accs_train,
        "accs_val": accs_val,
        "lr_W": lr_W_hist,
        "lr_C": lr_C_hist,
        "ema_W": ema_W_hist,
        "ema_C": ema_C_hist,
        "grad_norms_W": grad_norms_W_hist,
        "grad_norms_C": grad_norms_C_hist,}
      # Print epoch summary with loss, EMAs, and current learning rates